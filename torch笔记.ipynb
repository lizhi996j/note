{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转化为其它数据类型：\n",
    "- 如果是标量，使用.item()可转化；\n",
    "- 如果是向量，先使用.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor张量操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量维度的操作\n",
    "- tensor.expand(*sizes)  \n",
    "    - 这是一个整型序列，指定了想要“扩展”到的新形状（每个维度的大小）。\n",
    "    - 对应维度上，只有原来是 1 的维度才可以被扩展到更大的值; 写成 -1 表示“保持原来的大小”。\n",
    "- tensor.expand_as(other),相当于torch.expand(other.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2,3])\n",
    "a.unsqueeze(0)\n",
    "a.expand(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建矩阵\n",
    "- 创建单位阵： torch.eye(n)\n",
    "- 创建上三角阵： torch.triu(input, diagonal=0) (triangular upper)， \n",
    "  - input是另一个张量，不能只输入形状；\n",
    "  - diagonal=0表示对角线为0，diagonal=1表示从对角线上一行开始，diagonal=-1表示从对角线下一行开始；\n",
    "  - 上部值为1，下部值为0；\n",
    "- 创建下三角阵： torch.tril(input, diagonal=0) (triangular lower)\n",
    "- 创建对角阵：torch.diag(input, diagonal=0)\n",
    "    - input如果是向量，则创建对角阵，如果是二维矩阵则返回对角线，不支持高维\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [0, 2, 0, 0],\n",
      "        [0, 0, 3, 0],\n",
      "        [0, 0, 0, 4]])\n",
      "tensor([1, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "v = torch.tensor([1, 2, 3, 4])  # 一维张量\n",
    "D = torch.diag(v)\n",
    "print(D)\n",
    "\n",
    "M = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "d = torch.diag(M)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.tensordot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建两个张量\n",
    "a = torch.tensor([[1, 2], \n",
    "                [3, 4]])\n",
    "b = torch.tensor([[5, 6], \n",
    "                [7, 8]])\n",
    "\n",
    "# 在最后一个维度上进行点积; 只指定一个维度时是第一个张量的后n个维度与第二个张量的前n个维度做内积\n",
    "result = torch.tensordot(a, b, dims=1) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "二维矩阵在最后维度上的点积:\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "# 二维矩阵在最后维度上的点积\n",
    "a = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "b = torch.tensor([[5, 6],\n",
    "                  [7, 8]])\n",
    "c = torch.tensordot(a, b, dims=1)\n",
    "print(\"\\n二维矩阵在最后维度上的点积:\")\n",
    "print(c)\n",
    "# 输出:\n",
    "# tensor([[17, 23],\n",
    "#         [39, 53]])\n",
    "\n",
    "# 详细计算过程:\n",
    "# c[0,0] = a[0,:]·b[0,:] = (1*5 + 2*7) = 17\n",
    "# c[0,1] = a[0,:]·b[1,:] = (1*6 + 2*8) = 23\n",
    "# c[1,0] = a[1,:]·b[0,:] = (3*5 + 4*7) = 39\n",
    "# c[1,1] = a[1,:]·b[1,:] = (3*6 + 4*8) = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]],\n",
      "\n",
      "        [[16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[[160, 172, 184, 196],\n",
      "          [208, 220, 232, 244]],\n",
      "\n",
      "         [[184, 199, 214, 229],\n",
      "          [244, 259, 274, 289]],\n",
      "\n",
      "         [[208, 226, 244, 262],\n",
      "          [280, 298, 316, 334]],\n",
      "\n",
      "         [[232, 253, 274, 295],\n",
      "          [316, 337, 358, 379]]],\n",
      "\n",
      "\n",
      "        [[[448, 496, 544, 592],\n",
      "          [640, 688, 736, 784]],\n",
      "\n",
      "         [[472, 523, 574, 625],\n",
      "          [676, 727, 778, 829]],\n",
      "\n",
      "         [[496, 550, 604, 658],\n",
      "          [712, 766, 820, 874]],\n",
      "\n",
      "         [[520, 577, 634, 691],\n",
      "          [748, 805, 862, 919]]]])\n",
      "torch.Size([2, 4, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(24).reshape(2, 3, 4)  # shape: (2,3,4)\n",
    "b = torch.arange(24).reshape(3, 2, 4)  # shape: (3,2,4)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "# 在指定维度上进行张量点积\n",
    "# dims=([1],[0]) 表示a的第1维(大小为3)和b的第0维(大小为3)进行收缩\n",
    "result = torch.tensordot(a, b, dims=([1],[0]))\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 张量\n",
      ":tensor([[[ 0.3984,  0.3069, -1.0578,  0.7269],\n",
      "         [ 0.2161,  0.7968,  1.0643,  1.4901],\n",
      "         [-0.5727,  2.4418,  0.1298,  0.4969]],\n",
      "\n",
      "        [[-0.1357, -0.1951,  0.7969, -0.2274],\n",
      "         [ 0.0797,  1.1234, -0.1277,  2.3351],\n",
      "         [-1.4290, -0.6711, -1.5325, -0.7372]]])\n",
      "b 张量\n",
      ":tensor([[[-0.1563, -1.4337, -0.1802],\n",
      "         [ 0.4768, -1.4177, -0.0887],\n",
      "         [-0.2966, -1.0002, -1.5477],\n",
      "         [ 1.4982,  2.5524, -0.9826]],\n",
      "\n",
      "        [[ 0.7916,  0.1740,  0.0315],\n",
      "         [-2.0614,  0.0284, -0.9232],\n",
      "         [ 1.0376,  0.4370, -0.3258],\n",
      "         [-0.3902, -0.0320, -0.6299]]])\n",
      "result 张量\n",
      ":tensor([[ 2.6971,  2.2334,  0.8834],\n",
      "        [-1.0334,  1.2148, -5.6850],\n",
      "        [ 0.9095, -2.4160,  0.7360]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4)\n",
    "b = torch.randn(2, 4, 3)\n",
    "print(f\"a 张量\\n:{a}\")\n",
    "print(f\"b 张量\\n:{b}\")\n",
    "# 指定a的第0和2维，b的第0和1维收缩\n",
    "result = torch.tensordot(a, b, dims=([0, 2], [0, 1]))\n",
    "print(f\"result 张量\\n:{result}\")\n",
    "print(result.shape)  # torch.Size([3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.einsum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " einsum与tensordot的不同\n",
    " 1. 从输出结果看出，einsum的结果是特tensordot的特例，（tensordot返回的第一个元素的第一列和第二个元素的第二列是einsum的第一行和第二行，这是因为tensordot循环了所有结果，einsum只循环了指定结果。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 14,  20],\n",
      "         [ 32,  47],\n",
      "         [ 50,  74],\n",
      "         [ 68, 101],\n",
      "         [ 86, 128]],\n",
      "\n",
      "        [[ 20,  29],\n",
      "         [ 38,  56],\n",
      "         [ 56,  83],\n",
      "         [ 74, 110],\n",
      "         [ 92, 137]]])\n",
      "tensor([[ 14,  32,  50,  68,  86],\n",
      "        [ 29,  56,  83, 110, 137]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 批量矩阵-向量乘法，但只在特定维度\n",
    "batch = torch.tensor([[[1, 2, 3],\n",
    "                      [4, 5, 6], \n",
    "                      [7, 8, 9],\n",
    "                      [10, 11, 12],\n",
    "                      [13, 14, 15]],\n",
    "                     [[2, 3, 4],\n",
    "                      [5, 6, 7],\n",
    "                      [8, 9, 10], \n",
    "                      [11, 12, 13],\n",
    "                      [14, 15, 16]]])  # shape: (2,5,3)\n",
    "vectors = torch.tensor([[1, 2, 3],\n",
    "                       [2, 3, 4]])  # shape: (2,3)\n",
    "# 对每个批次，将最后一个维度相乘\n",
    "result1 = torch.tensordot(batch, vectors, dims=([2], [1]))\n",
    "\n",
    "print(result1)\n",
    "result2 = torch.einsum('bij,bj->bi', batch, vectors)\n",
    "print(result2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "einsum求和的外积：  \n",
    "m维向量$\\vec{a}$和n维向量$\\vec{b}$的外积：\n",
    "$\\vec{a}*\\vec{b}^T$  \n",
    "中的元素为:$c_{ij}=a_i*b_j$,  \n",
    "用爱因斯坦求和写为：\"i,j->ij\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量a: tensor([1, 2, 3])\n",
      "向量b: tensor([4, 5])\n",
      "外积结果:\n",
      "tensor([[ 4,  5],\n",
      "        [ 8, 10],\n",
      "        [12, 15]])\n"
     ]
    }
   ],
   "source": [
    "# 举例说明外积\n",
    "a = torch.tensor([1, 2, 3])  # 3维向量\n",
    "b = torch.tensor([4, 5])     # 2维向量\n",
    "\n",
    "# 使用einsum计算外积\n",
    "c = torch.einsum('i,j->ij', a, b)\n",
    "\n",
    "print(\"向量a:\", a)\n",
    "print(\"向量b:\", b) \n",
    "print(\"外积结果:\")\n",
    "print(c)\n",
    "# 结果是一个3x2的矩阵，其中c[i,j] = a[i] * b[j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.repeat(*sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "        [1, 2, 3, 1, 2, 3, 1, 2, 3]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量\n",
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "x.repeat(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.split(tensor, split_size_or_sections, dim=0)\n",
    "- tensor：要分割的张量\n",
    "- split_size_or_sections：  \n",
    "如果是整数，表示每块的大小  \n",
    "如果是列表或元组，表示每块的具体大小\n",
    "- dim：在哪个维度上分割，默认是第0维\n",
    "- 返回的是元组\n",
    "- 与np.split不同的是np.split的indices_or_sections参数是按按索引列表切分的；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9]))\n"
     ]
    }
   ],
   "source": [
    "#split_size_or_sections是整数情况\n",
    "import torch\n",
    "\n",
    "x = torch.arange(10)\n",
    "print(\"原始张量:\", x)  # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "# 按3分割\n",
    "parts = torch.split(x, 3)\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1]), tensor([2, 3, 4, 5]), tensor([6, 7, 8, 9]))\n"
     ]
    }
   ],
   "source": [
    "#split_size_or_sections是列表情况\n",
    "import torch\n",
    "x = torch.arange(10)\n",
    "parts = torch.split(x, [2, 4, 4])\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "(tensor([[0, 1],\n",
      "        [4, 5],\n",
      "        [8, 9]]), tensor([[ 2,  3],\n",
      "        [ 6,  7],\n",
      "        [10, 11]]))\n"
     ]
    }
   ],
   "source": [
    "#多维情况，要指定划分维度\n",
    "import torch\n",
    "x = torch.arange(12).reshape(3, 4)\n",
    "print(\"原始张量:\\n\", x)\n",
    "# tensor([[ 0,  1,  2,  3],\n",
    "#         [ 4,  5,  6,  7],\n",
    "#         [ 8,  9, 10, 11]])\n",
    "\n",
    "# 按列分割，每块2列\n",
    "parts = torch.split(x, 2, dim=1)\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.mul和torch.matmul的区别  \n",
    "torch.mul是逐元素相乘，torch.matmul是矩阵乘法。  \n",
    "torch.mm只适用于二维矩阵，torch.matmul支持批量矩阵乘法\n",
    "- **示例 1：基本批量矩阵乘法**\n",
    "\n",
    "   ```python\n",
    "   import torch\n",
    "\n",
    "   # 张量 A，形状为 [batch_size, M, K]\n",
    "   A = torch.randn(10, 3, 4)  # 10 个 3x4 的矩阵\n",
    "\n",
    "   # 张量 B，形状为 [batch_size, K, N]\n",
    "   B = torch.randn(10, 4, 5)  # 10 个 4x5 的矩阵\n",
    "\n",
    "   # 执行批量矩阵乘法\n",
    "   C = torch.matmul(A, B)  # 结果形状为 [10, 3, 5]\n",
    "   print(C.shape)\n",
    "   # 输出：torch.Size([10, 3, 5])\n",
    "   ```\n",
    "\n",
    "   - **示例 2：广播批量维度**\n",
    "\n",
    "   ```python\n",
    "   import torch\n",
    "\n",
    "   # 张量 A，形状为 [batch_size_A, M, K]\n",
    "   A = torch.randn(2, 3, 4)  # 2 个 3x4 的矩阵\n",
    "\n",
    "   # 张量 B，形状为 [K, N]\n",
    "   B = torch.randn(4, 5)     # 单个 4x5 的矩阵\n",
    "\n",
    "   # B 的批量维度将被广播为 2\n",
    "   C = torch.matmul(A, B)  # 结果形状为 [2, 3, 5]\n",
    "   print(C.shape)\n",
    "   # 输出：torch.Size([2, 3, 5])\n",
    "   ```\n",
    "\n",
    "   在这个示例中，`B` 的形状被视为 `[1, 4, 5]`，然后在第一个维度上广播为 `2`，以匹配 `A` 的批量维度。\n",
    "\n",
    "   - **示例 3：高维批量矩阵乘法**\n",
    "\n",
    "   ```python\n",
    "   import torch\n",
    "\n",
    "   # 张量 A，形状为 [D1, D2, M, K]\n",
    "   A = torch.randn(2, 3, 4, 5)\n",
    "\n",
    "   # 张量 B，形状为 [D1, D2, K, N]\n",
    "   B = torch.randn(2, 3, 5, 6)\n",
    "\n",
    "   # 结果形状为 [2, 3, 4, 6]\n",
    "   C = torch.matmul(A, B)\n",
    "   print(C.shape)\n",
    "   # 输出：torch.Size([2, 3, 4, 6])\n",
    "   ```\n",
    "\n",
    "   在这里，`A`和 `B` 都有批量维度 `[2, 3]`，`torch.matmul` 会在这两个维度上逐元素地执行矩阵乘法。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10,  40],\n",
      "        [ 90, 160]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[10, 20], [30, 40]])\n",
    "\n",
    "result = torch.mul(a, b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "result = torch.matmul(a, b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.pdist(input,p)\n",
    "计算成对距离；p指定范数，input是二维[N,D],返回结果是距离矩阵的上三角部分按顺序输出；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础应用:\n",
    "知道基础知识后最重要的点就是：\n",
    "- unique\n",
    "- 计数\n",
    "- 排序\n",
    "- 采样\n",
    "- 选择与替换\n",
    "- 向量操作（对行列的整体操作）\n",
    "- 自定义函数的应用\n",
    "- 与其它数据类型转化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unique\n",
    "- torch.unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None)  \n",
    "    - 没有return_index选项，但可以使用如下方法获得元素第一次出现时的索引：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 8, 3, 5, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.randint(0,4,(10,2))\n",
    "unique,inverse = torch.unique(input, return_inverse=True,dim=0)\n",
    "src = torch.arange(len(input))\n",
    "unique_index = torch.full((len(unique),),len(input),dtype=torch.int64)\n",
    "unique_index.scatter_reduce_(0,inverse,src,reduce='min') # min似乎会和原来tensor值比,所以需要index原来的值为最大值；\n",
    "print(unique_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 排序\n",
    "- torch.sort(input, dim=-1, descending=False, stable=False)\n",
    "- torch.argsort(input,dim=-1,descending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 采样\n",
    "- torch.randperm(n,)\n",
    "生成随机排列索引的常用函数。它会返回一个包含 [0, 1, 2, …, n-1] 且完全随机打乱顺序的整数张量，常用于**无放回抽样**、打乱数据顺序等场景\n",
    "- torch.multinomial(input, num_samples, replacement=False)\n",
    "  - input (Tensor)\n",
    "    一维或二维张量，表示每个元素（或类别）的非负权重（概率权重）。  \n",
    "    - 如果是一维张量，形状为 (N,)，表示从 0…N-1 中采样。\n",
    "    - 如果是二维张量，形状为 (B, N)，则对每行单独采样，返回形状 (B, num_samples)。\n",
    "  - num_samples (int)\n",
    "    要抽取的样本数量。\n",
    "  - replacement (bool, optional, 默认 False)\n",
    "    - False：无放回抽样——抽到某索引后不会再次被抽中。\n",
    "    - True ：有放回抽样——每一次抽样都基于同一分布，不论之前是否抽中过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 权重可以自动归一化\n",
    "import torch\n",
    "weights = torch.tensor([1,2,3],dtype=float)\n",
    "torch.multinomial(weights, 2, replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 适用torch.randperm()进行不放回抽样的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9092, 0.6047, 0.1946, 0.5106, 0.4763, 0.1430, 0.6893, 0.9243, 0.1575,\n",
      "        0.6141])\n"
     ]
    }
   ],
   "source": [
    "# 从100个数里抽10个\n",
    "import torch\n",
    "data = torch.rand(100)\n",
    "sample_index = torch.randperm(100)[:10]\n",
    "sampled_data = data[sample_index]\n",
    "print(sampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将源张量的值根据某种规则聚合到目标张量中： tensor.scatter_reduce_(dim, index, src, reduce, *, include_self=True)\n",
    "| 参数名 | 说明 |\n",
    "|--------|------|\n",
    "| tensor | 目标张量，会被原地修改。 |\n",
    "| dim | 进行操作的维度。 |\n",
    "| index | 一个张量，指定 src 中的每个元素在 tensor 的哪个位置聚合。必须和 src 形状相同。 |\n",
    "| src | 源张量，包含要聚合的值。 |\n",
    "| reduce | 字符串：\"sum\"、\"prod\"、\"mean\"、\"amax\"、\"amin\" 等。表示用哪种方式聚合。 |\n",
    "| include_self | 如果为 True，目标张量中原有的值也参与归约；否则先清零再聚合。 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 按索引求和\n",
    "import torch\n",
    "\n",
    "# 初始目标张量\n",
    "out = torch.zeros(5)\n",
    "\n",
    "# 索引张量\n",
    "index = torch.tensor([0, 1, 1, 2, 3])\n",
    "\n",
    "# 源张量\n",
    "src = torch.tensor([10, 20, 30, 40, 50])\n",
    "\n",
    "# 原地 scatter + reduce 操作：将 src 中的值根据 index 添加到 out 中\n",
    "out.scatter_reduce_(0, index, src, reduce=\"sum\", include_self=True)\n",
    "\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择/替换张量指定索引/指定条件的元素\n",
    "- 在很多情况下，这两者可以用整数索引代替\n",
    "- 一个使用tensor.scatter_的独特例子见 “一些场景”部分，主要是利用reduce参数；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 选择tensor中指定索引的元素\n",
    "- 整数索引\n",
    "- tensor.gather(input,dim,index):  \n",
    "    - index (LongTensor)：索引张量，其形状必须与输入张量在除了指定维度以外的维度保持一致\n",
    "    - **以上约束使得返回张量的形状与input只在指定维度dim有不同**，例如以下例子中取的张量第0维度肯定是2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.8000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 假设有2个样本，每个样本有4个类别的得分\n",
    "logits = torch.tensor([[0.1, 0.5, 0.3, 0.2],\n",
    "                       [0.7, 0.2, 0.1, 0.8]])\n",
    "# 每个样本的预测类别（注意每个样本不同）\n",
    "pred = torch.tensor([1, 3])\n",
    "torch.gather(logits,1,pred.unsqueeze(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 选择tensor中满足特定条件的元素\n",
    "- torch.where(condition,x,y):\n",
    "    - 注意只有condition条件时返回满足条件的坐标，有x,y时返回值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2]), tensor([0, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "# 返回坐标\n",
    "x = torch.tensor([[1, 0, 0],\n",
    "                  [0, 2, 0],\n",
    "                  [0, 0, 3]])\n",
    "\n",
    "indices = torch.where(x > 0)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 20,  3, 40])\n"
     ]
    }
   ],
   "source": [
    "# 返回值\n",
    "a = torch.tensor([1, 2, 3, 4])\n",
    "b = torch.tensor([10, 20, 30, 40])\n",
    "cond = torch.tensor([True, False, True, False])\n",
    "\n",
    "# 三个参数：返回值\n",
    "out = torch.where(cond, a, b)\n",
    "print(out)\n",
    "# 输出: tensor([ 1, 20,  3, 40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 替换张量指定维度的元素值\n",
    "- tensor.scatter_(dim,index,src) 或 tensor.scatter(dim,index,src)\n",
    "    - 下划线表示是否原地操作；\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 写代码时的疑问"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用torch把标签转化为one-hot编码\n",
    "- 自动实现： torch.functional.one_hot(labels,num_classes)\n",
    "- 手动实现：Tensor.scatter_(dim, index, src, reduce='none')  \n",
    "把src中的值放到index指定的tensor位置中；reduce用来处理当多个源值要写入同一目标时的行为；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "labels = torch.tensor([0,1,2,3])\n",
    "one_hot = F.one_hot(labels,num_classes=4)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_classes = 4\n",
    "labels = torch.tensor([0,2,2,3])\n",
    "one_hot = torch.zeros(labels.size(0),num_classes)\n",
    "one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN的pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)和pad_packed_sequence(sequence,batch_first=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于torch.nn.BCEloss： 二分类或者多分类多标签(n个类的二分类任务)\n",
    "~~1. 可以用在numpy吗？数据类型可以是long吗？~~  \n",
    "~~不行，数据类型必须是float~~\n",
    "1. input和target的位置不能调换，第一个是input,第二个是target,  \n",
    "BCELoss input和target数据类型都是float;\n",
    "3. 关于input和target的维度:相同，任意\n",
    "4. 需要先初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.3228)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "criterion = torch.nn.BCELoss()\n",
    "label = torch.randint(0,2,(10,))\n",
    "print(label.dtype)\n",
    "pred = torch.tensor(np.random.rand(10),dtype=torch.float32)\n",
    "criterion(pred, label.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于torch.nn.CrossEntropyLoss()\n",
    "1. 输入数据类型: preds必须是float,target必须是long,除非目标是每个样本属于每个类的概率\n",
    "2. 维度要求：preds的维度是(batch_size,num_classes, d1, d2, d3, ...),target的维度是(batch_size,d1, d2, d3, ...);\n",
    "3. 需要先初始化\n",
    "4. CrossEntropyLoss适用于互斥的多类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6657)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "batch_size, seq_len, vocab_size = 10, 5, 5\n",
    "preds = torch.rand(batch_size, vocab_size, seq_len)\n",
    "labels = torch.randint(0,2,size=(batch_size,seq_len), dtype=torch.long)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(preds, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遍历一个模块的子模块，参数，缓冲区\n",
    "- .named_modules() ：\n",
    "  - 返回一个子模块名称和指向该子模块的引用；（注意第一个返回的是这个模块）\n",
    "  - 也会返回子模块的子模块\n",
    "- .named_parameters() ：返回一个参数名称和指向该参数的引用；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.weight: torch.Size([20, 10])\n",
      "encoder.bias: torch.Size([20])\n",
      "decoder.weight: torch.Size([5, 20])\n",
      "decoder.bias: torch.Size([5])\n",
      "sub_module.linear.weight: torch.Size([20, 10])\n",
      "sub_module.linear.bias: torch.Size([20])\n",
      "--------------------------------\n",
      ": MyNet\n",
      "encoder: Linear\n",
      "relu: ReLU\n",
      "decoder: Linear\n",
      "sub_module: sub_module\n",
      "sub_module.linear: Linear\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class sub_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(10, 20)   # 子模块\n",
    "        self.relu = nn.ReLU()\n",
    "        self.decoder = nn.Linear(20, 5)\n",
    "        self.sub_module = sub_module()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "mynet = MyNet()\n",
    "\n",
    "for name, param in mynet.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "for name, module in mynet.named_modules():\n",
    "    print(f\"{name}: {module.__class__.__name__}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存和加载模型\n",
    "- 有两种方式，一种是保存整个模型，一种是保存模型状态的字典；\n",
    "- 保存状态字典的方法是：\n",
    "```python\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "a = torch.load('model.pth')\n",
    "model.load_state_dict(a)\n",
    "```\n",
    "- 保存整个模型的方法是：\n",
    "```python\n",
    "torch.save(model,'entire_model.pth')\n",
    "model = torch.load('entire_model.pth')\n",
    "```\n",
    "- 除此之外还看到有保存checkpoint的：\n",
    "1. checkpoint\n",
    "```python\n",
    "torch.save({\n",
    "    'epoch': 10,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "}, 'checkpoint.pth') \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看显存管理\n",
    "torch.cuda.empty_cache():清空分配的显存  \n",
    "torch.cuda.memory_summary(device=None):查看显存使用情况  \n",
    "torch.cuda.memory_allocated(device=None):查看当前分配的显存  \n",
    "torch.cuda.memory_reserved(device=None):查看当前保留的显存  \n",
    "device用来指定设备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多进程\n",
    "torch.multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于cuda\n",
    "三种gpu共享模型：\n",
    "- MIG(multi instance gpu): 将一个gpu切分成数个实例；\n",
    "- MPS(multi process service)：\n",
    "- compute mode:default\n",
    "不知道是啥：MPI 作业（MPI Job）指的是使用 Message Passing Interface（消息传递接口，简称 MPI）标准来执行的并行计算任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启用MPS:  \n",
    "- sudo nvidia-cuda-mps-control -d 这里的-d是daemon,既以守护进程的方式运行；\n",
    "参看MPS server和client:\n",
    "- echo get_server_list | nvidia-cuda-mps-control\n",
    "- echo get_client_list <server_id>| nvidia-cuda-mps-control\n",
    "\n",
    "停止MPS:\n",
    "- echo quit | nvidia-cuda-mps-control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MPS 中一个进程报错是否会导致所有进程报错？  \n",
    "MPS（Multi-Process Service）并不总是 \"一个进程报错就导致所有进程报错\".\n",
    "1. MPS 的核心机制回顾\n",
    "MPS 让多个进程共享同一个 CUDA 上下文（GPU \"工作环境\"），而不是每个进程独立一个。  \n",
    "优势：减少内存开销，提高并发效率。  \n",
    "潜在缺点：共享意味着一个进程的资源密集错误（如 OOM）可能污染整个上下文，影响其他进程。  \n",
    "来自 NVIDIA 官方文档（CUDA Toolkit）：MPS 提供隔离，但不保证完全独立；如果上下文崩溃，整个 MPS 服务器可能受影响，导致客户端（进程）失败。  \n",
    "2. 什么时候一个进程报错会导致所有进程报错？  \n",
    "不是所有错误都会连锁\n",
    "- 非致命错误（如计算逻辑错误、loss NaN）：通常只影响该进程，其他进程继续运行。MPS 会隔离这些。  \n",
    "示例：如果一个进程在计算中除以零，只那个进程抛异常，其他进程不受影响。  \n",
    "- 会连锁的错误类型（如您的案例）：  \n",
    "    - 内存相关错误 (OOM, illegal access)：是的，这些往往导致连锁失败。  \n",
    "原因：所有进程共享内存池。如果一个进程尝试分配过多内存（导致 OOM），它可能触发 GPU 驱动的保护机制，重置或污染上下文。结果其他进程尝试访问共享资源时，也会遇到 \"illegal memory access\"（非法内存访问）。   \n",
    "    - 其他连锁场景：    \n",
    "        - GPU 内核崩溃（kernel panic）。  \n",
    "        - 超时或驱动 bug。  \n",
    "        - 高负载下，MPS 服务器崩溃（罕见，但可能）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些cuda命令\n",
    "nvidia-smi -q: q是query的意思，用于查看详细信息；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Hello World\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
