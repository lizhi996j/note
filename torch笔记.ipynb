{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转化为其它数据类型：\n",
    "- 如果是标量，使用.item()可转化；\n",
    "- 如果是向量，先使用.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 代数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建矩阵\n",
    "- 创建单位阵： torch.eye(n)\n",
    "- 创建上三角阵： torch.triu(input, diagonal=0) (triangular upper)， \n",
    "  - input是另一个张量，不能只输入形状；\n",
    "  - diagonal=0表示对角线为0，diagonal=1表示从对角线上一行开始，diagonal=-1表示从对角线下一行开始；\n",
    "  - 上部值为1，下部值为0；\n",
    "- 创建下三角阵： torch.tril(input, diagonal=0) (triangular lower)\n",
    "- 创建对角阵：torch.diag(input, diagonal=0)\n",
    "    - input如果是向量，则创建对角阵，如果是二维矩阵则返回对角线，不支持高维\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [0, 2, 0, 0],\n",
      "        [0, 0, 3, 0],\n",
      "        [0, 0, 0, 4]])\n",
      "tensor([1, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "v = torch.tensor([1, 2, 3, 4])  # 一维张量\n",
    "D = torch.diag(v)\n",
    "print(D)\n",
    "\n",
    "M = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "d = torch.diag(M)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一些函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.tensordot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建两个张量\n",
    "a = torch.tensor([[1, 2], \n",
    "                [3, 4]])\n",
    "b = torch.tensor([[5, 6], \n",
    "                [7, 8]])\n",
    "\n",
    "# 在最后一个维度上进行点积; 只指定一个维度时是第一个张量的后n个维度与第二个张量的前n个维度做内积\n",
    "result = torch.tensordot(a, b, dims=1) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "二维矩阵在最后维度上的点积:\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "# 二维矩阵在最后维度上的点积\n",
    "a = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "b = torch.tensor([[5, 6],\n",
    "                  [7, 8]])\n",
    "c = torch.tensordot(a, b, dims=1)\n",
    "print(\"\\n二维矩阵在最后维度上的点积:\")\n",
    "print(c)\n",
    "# 输出:\n",
    "# tensor([[17, 23],\n",
    "#         [39, 53]])\n",
    "\n",
    "# 详细计算过程:\n",
    "# c[0,0] = a[0,:]·b[0,:] = (1*5 + 2*7) = 17\n",
    "# c[0,1] = a[0,:]·b[1,:] = (1*6 + 2*8) = 23\n",
    "# c[1,0] = a[1,:]·b[0,:] = (3*5 + 4*7) = 39\n",
    "# c[1,1] = a[1,:]·b[1,:] = (3*6 + 4*8) = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]],\n",
      "\n",
      "        [[16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[[160, 172, 184, 196],\n",
      "          [208, 220, 232, 244]],\n",
      "\n",
      "         [[184, 199, 214, 229],\n",
      "          [244, 259, 274, 289]],\n",
      "\n",
      "         [[208, 226, 244, 262],\n",
      "          [280, 298, 316, 334]],\n",
      "\n",
      "         [[232, 253, 274, 295],\n",
      "          [316, 337, 358, 379]]],\n",
      "\n",
      "\n",
      "        [[[448, 496, 544, 592],\n",
      "          [640, 688, 736, 784]],\n",
      "\n",
      "         [[472, 523, 574, 625],\n",
      "          [676, 727, 778, 829]],\n",
      "\n",
      "         [[496, 550, 604, 658],\n",
      "          [712, 766, 820, 874]],\n",
      "\n",
      "         [[520, 577, 634, 691],\n",
      "          [748, 805, 862, 919]]]])\n",
      "torch.Size([2, 4, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(24).reshape(2, 3, 4)  # shape: (2,3,4)\n",
    "b = torch.arange(24).reshape(3, 2, 4)  # shape: (3,2,4)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "# 在指定维度上进行张量点积\n",
    "# dims=([1],[0]) 表示a的第1维(大小为3)和b的第0维(大小为3)进行收缩\n",
    "result = torch.tensordot(a, b, dims=([1],[0]))\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 张量\n",
      ":tensor([[[ 0.3984,  0.3069, -1.0578,  0.7269],\n",
      "         [ 0.2161,  0.7968,  1.0643,  1.4901],\n",
      "         [-0.5727,  2.4418,  0.1298,  0.4969]],\n",
      "\n",
      "        [[-0.1357, -0.1951,  0.7969, -0.2274],\n",
      "         [ 0.0797,  1.1234, -0.1277,  2.3351],\n",
      "         [-1.4290, -0.6711, -1.5325, -0.7372]]])\n",
      "b 张量\n",
      ":tensor([[[-0.1563, -1.4337, -0.1802],\n",
      "         [ 0.4768, -1.4177, -0.0887],\n",
      "         [-0.2966, -1.0002, -1.5477],\n",
      "         [ 1.4982,  2.5524, -0.9826]],\n",
      "\n",
      "        [[ 0.7916,  0.1740,  0.0315],\n",
      "         [-2.0614,  0.0284, -0.9232],\n",
      "         [ 1.0376,  0.4370, -0.3258],\n",
      "         [-0.3902, -0.0320, -0.6299]]])\n",
      "result 张量\n",
      ":tensor([[ 2.6971,  2.2334,  0.8834],\n",
      "        [-1.0334,  1.2148, -5.6850],\n",
      "        [ 0.9095, -2.4160,  0.7360]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4)\n",
    "b = torch.randn(2, 4, 3)\n",
    "print(f\"a 张量\\n:{a}\")\n",
    "print(f\"b 张量\\n:{b}\")\n",
    "# 指定a的第0和2维，b的第0和1维收缩\n",
    "result = torch.tensordot(a, b, dims=([0, 2], [0, 1]))\n",
    "print(f\"result 张量\\n:{result}\")\n",
    "print(result.shape)  # torch.Size([3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.einsum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " einsum与tensordot的不同\n",
    " 1. 从输出结果看出，einsum的结果是特tensordot的特例，（tensordot返回的第一个元素的第一列和第二个元素的第二列是einsum的第一行和第二行，这是因为tensordot循环了所有结果，einsum只循环了指定结果。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 14,  20],\n",
      "         [ 32,  47],\n",
      "         [ 50,  74],\n",
      "         [ 68, 101],\n",
      "         [ 86, 128]],\n",
      "\n",
      "        [[ 20,  29],\n",
      "         [ 38,  56],\n",
      "         [ 56,  83],\n",
      "         [ 74, 110],\n",
      "         [ 92, 137]]])\n",
      "tensor([[ 14,  32,  50,  68,  86],\n",
      "        [ 29,  56,  83, 110, 137]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 批量矩阵-向量乘法，但只在特定维度\n",
    "batch = torch.tensor([[[1, 2, 3],\n",
    "                      [4, 5, 6], \n",
    "                      [7, 8, 9],\n",
    "                      [10, 11, 12],\n",
    "                      [13, 14, 15]],\n",
    "                     [[2, 3, 4],\n",
    "                      [5, 6, 7],\n",
    "                      [8, 9, 10], \n",
    "                      [11, 12, 13],\n",
    "                      [14, 15, 16]]])  # shape: (2,5,3)\n",
    "vectors = torch.tensor([[1, 2, 3],\n",
    "                       [2, 3, 4]])  # shape: (2,3)\n",
    "# 对每个批次，将最后一个维度相乘\n",
    "result1 = torch.tensordot(batch, vectors, dims=([2], [1]))\n",
    "\n",
    "print(result1)\n",
    "result2 = torch.einsum('bij,bj->bi', batch, vectors)\n",
    "print(result2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "einsum求和的外积：  \n",
    "m维向量$\\vec{a}$和n维向量$\\vec{b}$的外积：\n",
    "$\\vec{a}*\\vec{b}^T$  \n",
    "中的元素为:$c_{ij}=a_i*b_j$,  \n",
    "用爱因斯坦求和写为：\"i,j->ij\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量a: tensor([1, 2, 3])\n",
      "向量b: tensor([4, 5])\n",
      "外积结果:\n",
      "tensor([[ 4,  5],\n",
      "        [ 8, 10],\n",
      "        [12, 15]])\n"
     ]
    }
   ],
   "source": [
    "# 举例说明外积\n",
    "a = torch.tensor([1, 2, 3])  # 3维向量\n",
    "b = torch.tensor([4, 5])     # 2维向量\n",
    "\n",
    "# 使用einsum计算外积\n",
    "c = torch.einsum('i,j->ij', a, b)\n",
    "\n",
    "print(\"向量a:\", a)\n",
    "print(\"向量b:\", b) \n",
    "print(\"外积结果:\")\n",
    "print(c)\n",
    "# 结果是一个3x2的矩阵，其中c[i,j] = a[i] * b[j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.repeat(*sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "        [1, 2, 3, 1, 2, 3, 1, 2, 3]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量\n",
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "x.repeat(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.split(tensor, split_size_or_sections, dim=0)\n",
    "- tensor：要分割的张量\n",
    "- split_size_or_sections：  \n",
    "如果是整数，表示每块的大小  \n",
    "如果是列表或元组，表示每块的具体大小\n",
    "- dim：在哪个维度上分割，默认是第0维\n",
    "- 返回的是元组\n",
    "- 与np.split不同的是np.split的indices_or_sections参数是按按索引列表切分的；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9]))\n"
     ]
    }
   ],
   "source": [
    "#split_size_or_sections是整数情况\n",
    "import torch\n",
    "\n",
    "x = torch.arange(10)\n",
    "print(\"原始张量:\", x)  # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "# 按3分割\n",
    "parts = torch.split(x, 3)\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1]), tensor([2, 3, 4, 5]), tensor([6, 7, 8, 9]))\n"
     ]
    }
   ],
   "source": [
    "#split_size_or_sections是列表情况\n",
    "import torch\n",
    "x = torch.arange(10)\n",
    "parts = torch.split(x, [2, 4, 4])\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "(tensor([[0, 1],\n",
      "        [4, 5],\n",
      "        [8, 9]]), tensor([[ 2,  3],\n",
      "        [ 6,  7],\n",
      "        [10, 11]]))\n"
     ]
    }
   ],
   "source": [
    "#多维情况，要指定划分维度\n",
    "import torch\n",
    "x = torch.arange(12).reshape(3, 4)\n",
    "print(\"原始张量:\\n\", x)\n",
    "# tensor([[ 0,  1,  2,  3],\n",
    "#         [ 4,  5,  6,  7],\n",
    "#         [ 8,  9, 10, 11]])\n",
    "\n",
    "# 按列分割，每块2列\n",
    "parts = torch.split(x, 2, dim=1)\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.mul和torch.matmul的区别  \n",
    "torch.mul是逐元素相乘，torch.matmul是矩阵乘法。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10,  40],\n",
      "        [ 90, 160]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[10, 20], [30, 40]])\n",
    "\n",
    "result = torch.mul(a, b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "result = torch.matmul(a, b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遍历一个模块的子模块，参数，缓冲区\n",
    "- .named_modules() ：\n",
    "  - 返回一个子模块名称和指向该子模块的引用；（注意第一个返回的是这个模块）\n",
    "  - 也会返回子模块的子模块\n",
    "- .named_parameters() ：返回一个参数名称和指向该参数的引用；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.weight: torch.Size([20, 10])\n",
      "encoder.bias: torch.Size([20])\n",
      "decoder.weight: torch.Size([5, 20])\n",
      "decoder.bias: torch.Size([5])\n",
      "sub_module.linear.weight: torch.Size([20, 10])\n",
      "sub_module.linear.bias: torch.Size([20])\n",
      "--------------------------------\n",
      ": MyNet\n",
      "encoder: Linear\n",
      "relu: ReLU\n",
      "decoder: Linear\n",
      "sub_module: sub_module\n",
      "sub_module.linear: Linear\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class sub_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(10, 20)   # 子模块\n",
    "        self.relu = nn.ReLU()\n",
    "        self.decoder = nn.Linear(20, 5)\n",
    "        self.sub_module = sub_module()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "mynet = MyNet()\n",
    "\n",
    "for name, param in mynet.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "for name, module in mynet.named_modules():\n",
    "    print(f\"{name}: {module.__class__.__name__}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
